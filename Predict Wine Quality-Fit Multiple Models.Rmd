---
title: "Predict Wine Quality-Fit Multiple Models"
author: "Ming Chen"
date: "10/21/2021"
output: html_document
---

## 1. Load the data 
### 1.1 Read in the csv file and look at the dimension of the data
```{r}
library('psych')
dat <- read.table("winequality-red.csv", sep=";", header=TRUE)
dim(dat)
```
### 1.2 Preview the first few rows
```{r}
head(dat)
```
### 1.3 Generate summary statistics for each column
```{r}
summary(dat)[,1:8]
```
## 2. Define high quality wines
### 2.1 Look at the value counts for each wine quality
```{r}
table(dat$quality)
dat$quality <- I(dat$quality > 6) * 1
```
There is a huge gap between 6 and 7 so we decide to classify wine quality that are higher than 6 as high-quality wine.
### 2.2 Check the summary statistics for `quality` column again
```{r echo=False}
summary(dat$quality)
```
## 2.3 Split the data
```{r}
set.seed(42)
trn <- runif(nrow(dat)) < .7
train <- dat[trn==TRUE,]
test <- dat[trn==FALSE,]
```
## 3. Logistic regression
### Fit a logistic regression 
```{r }
glm <- glm(quality ~ ., family="binomial", data=train)
yhat_glm <- predict(glm, type="response")
```
### Evaluate prediction accuracy based on ROC
The area under the ROC curve(AUC) summarizes the overall performance of the classifier over all possible probability cutoffs to evaluate model performance. The higher the AUC, the better the model is at distinguishing between high-quality wines and low-quality wines.
```{r}
test$yhat.glm <- predict(glm, test, type="response")
glm.roc <- roc(test$quality, test$yhat.glm, direction="<")
glm.roc
```
## 4. Fit a LDA and generate a ROC
```{r}
library(MASS)
lda <- lda(quality ~ ., data=train)
library(pROC)
yhat_lda <- predict(lda)$posterior[,2]
test$yhat.lda <- predict(lda, test)$posterior[,2]
lda.roc <- roc(test$quality, test$yhat.lda, direction="<")
lda.roc
```
## 5. Fit a QDA and generate a ROC
```{r}
qda <- qda(quality ~ ., data=train)
yhat_qda <- predict(qda)$posterior[,2]
test$yhat.qda <- predict(qda, test)$posterior[,2]
qda.roc <- roc(test$quality, test$yhat.qda, direction="<")
qda.roc
```

## 6. Classification Tree

### 6.1 Fit a classification tree and plot the tree
```{r}
Y_test <-test[,12]
library(rpart)
form1 <- formula(quality~.)
t1 <- rpart(form1, data=train, cp=.001, method="class")
plot(t1,uniform=T,compress=T,margin=.05,branch=0.3)
text(t1, cex=.7, col="navy",use.n=TRUE)
```
Here we use ‘rpart’ function to fit a classification tree since it automatically performs a cross validation, which allows us to pruning a tree model by choosing a different parameter ‘cp’. 
### 6.2 Evaluate prediction accuracy based on ROC
```{r}
yhat.t1 <- predict(t1, test, type="prob")[,2]
t1.roc <- roc(Y_test, yhat.t1, direction="<")
t1.roc
```
As we can see, the ROC is about 0.8, suggesting good performance of the tree model. 
### 6.3 Choose a proper size of the tree
The t1 model is somewhat complex. What we want is to find a balance between simplicity and complexicty. So we'dbetter pruning the tree model to get a simpler model.Let’s look at other choices of ‘cp’ parameters in the cross-validated prediction.
```{r}
plotcp(t1)
CP <- printcp(t1)
cp <- CP[,1][CP[,2] == 13]
cp
```
According to the documentation, "a good choice of cp for pruning is often the leftmost value for which the mean lies below the horizontal line". But we want to do mild tuning instead of choosing the second cp with the tree size of just 3, we want to choose the third cp, which hasa resonable tree size.
### 6.4 Do mild pruning to reduce the complexity 
```{r}
t2 <- prune(t1,cp=cp[1])
plot(t2,uniform=T,compress=T,margin=.05,branch=0.3)
text(t2, cex=.7, col="navy",use.n=TRUE)
```
### 6.5 Evaluate prediction accuracy based on ROC
```{r}
yhat.t2 <- predict(t2, test, type="prob")[,2]
library("pROC")
t2.roc <- roc(Y_test, yhat.t2, direction="<")
t2.roc
```
## 7. Random forest
### 7.1 Make sure X and Y are in right format
First, we need to convert the feature vaiables to a matrix in order to use the `randomForest` function. Similarly, since `quality` is a categorical variable, we'll also convert it to `factor` data structure.
```{r}
library(randomForest)
xvars <- names(dat)[-12]
X <- as.matrix(train[,xvars])
Y <- factor(train$quality)
```
### 7.2 Split dataset into training (70%) and test dataset (30%)
```{r}
X.trn <- train[,-12]
X.tst <- test[,-12]
Y.trn <- train[,12]
Y.tst <- test[,12]
xvars <- names(dat)[-12]
```
### 7.3 Fit a random forest and generate a summary of the model
```{r}
set.seed(652)
rf1 <- randomForest(x=X, y=Y, ntree=500, mtry=round(ncol(X)^.5) , importance=TRUE)
summary(rf1)
```
Now we have a random forest model. But unlike GLM or GAM, the result doesn’t give us the coefficient of each feature. This is because random forest make predictions by averaging all predictions generated by a number of decision trees built on boostrapped training samples. 

### 7.4 Look at the influence of each feature  
Although the collection of bagged trees is difficult to interpret, we can still understand what drives the predictions by obtaining a summary of the importance of each predictor. 
```{r}
importance(rf1)
```
The former metric is based on the mean decrease of accuracy in predictions on the out-of-bag samples when excluding a given variable from the model. The later metric is based on the decrease of total amount of Gini index when splitting over a given predictor. The higher the value of mean decrease accuracy or mean decrease Gini score, the higher the importance of the variable in the model. we can see that alcohol is the most important predictors.
### 7.5 Plot the importance metrics
```{r}
varImpPlot(rf1)
```
### 7.6 Evaluate the prediction accuracy
```{r out.height=5, out.height=5}
library("pROC")
yhat.rf1 <- predict(rf1, test, type="prob")[,2]
rf1.roc <- roc(test$quality, yhat.rf1, direction="<")
rf1.roc
```
As we can see, the AUC is 0.93, which is very high. It turns out that random forest performs far better than all previous model we fitted before in terms of AUC!

## 8. ADA Boosting
Now we want to fit an Ada boosting model to see if we can further improve the model.
### 8.1 Fit an ADA Boosting
```{r}
library(gbm)
abt1 <- gbm(quality~., data = train, distribution = "adaboost", n.trees = 500, interaction.depth=6, shrinkage = 0.005)
summary(abt1)
```
The bar chart clearly shows the relative importance of each predictors, which is quite similar to the result generated from random forest.

### 8.2 Evaluate prediction accuracy
```{r}
abt1.roc <- roc(test$quality, yhat.abt1, direction="<")
abt1.roc
```
The AUC is 90.9%, which is slightly lower than that of the random forest (92.9%). This is because ada boosting tends to overfitting in the process of fitting different subtrees sequentially, thus resulting a lower out-of-sample prediction performance. 
## 9. Compare the ROC for all models
```{r}
plot(rf1.roc, lwd=1, col="darkolivegreen")
lines(abt1.roc, lwd=1, col="darkolivegreen3")
lines(t2.roc,lwd=1, col="darkolivegreen1")
lines(glm.roc, lwd=1, col = "brown")
lines(lda.roc, lwd=1, col = "brown3")
lines(qda.roc, lwd=1, col = "orange")
legend("bottomright",title="ROC Curves",     c("RandomForest","Boosting","SingleTree","glm","lda","qda"), 
       fill=c("darkolivegreen", "darkolivegreen3","darkolivegreen1", "brown","brown3","orange"))
```
When we compare the ROC curve of all the models we generated before, it turns out that random forest is the best one. However, the more advanced learners (randomforest & adaboost) improve the model at the expense of interpretability. In the situation where I only have two models (adaboosting and glm) and have to make choice between the two,  I might choose glm because I don’t want to lose interpretability of the model for the sake of little improvement in AUC.
